# import os
# from transformers import pipeline
# from dotenv import load_dotenv

# load_dotenv()

# def load_model(model_name):
#     model = pipeline(model=model_name, use_auth_token=os.getenv("HUGGINGFACE_TOKEN"))
#     return model


# def get_model_response(model_name, prompt):
#     model = load_model(model_name)
#     response = model(prompt, max_length=200, do_sample=True, top_p=0.95, num_return_sequences=1)
#     return response[0]['generated_text']
import os
from transformers import T5Tokenizer, T5ForConditionalGeneration
from dotenv import load_dotenv

load_dotenv()

def load_model(model_name):
    # Load the tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained(model_name, token=os.getenv("HUGGINGFACE_TOKEN"))
    model = T5ForConditionalGeneration.from_pretrained(model_name, token=os.getenv("HUGGINGFACE_TOKEN"))
    return tokenizer, model

def get_model_response(model_name, prompt):
    tokenizer, model = load_model(model_name)

    # Encode the input prompt
    input_ids = tokenizer.encode(prompt, return_tensors="pt")

    # Generate the response from the model
    outputs = model.generate(
        input_ids, 
        max_length=200, 
        do_sample=True, 
        top_p=0.95, 
        num_return_sequences=1
    )

    # Decode the generated tokens to get the output text
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return response_text
